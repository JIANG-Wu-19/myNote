{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Neural Networks & Deep Learning**\n",
    "\n",
    "> 欢迎同学们观看哔哩哔哩[《数之道》相关视频](https://www.bilibili.com/video/BV12b4y1X7Wv/?spm_id_from=333.788&vd_source=2dbf3ce169def83a8e8b53329f7d2135),提升自己对神经网络的理解。\n",
    "\n",
    "在这个作业中，我们的任务是构建一个神经网络来识别手写数字图片，本质上是一个类别数为10的图像分类问题。首先我们会搭建一个简单的**神经网络**，然后分别实现**AdaGrad、RMSprop、Momentum、Nesterov Momentum和Adam**优化器来加速模型收敛。 \n",
    "需要修改代码主要在第三部分：\n",
    "- 搭建三层MLP网络模型\n",
    "- **实现各种优化器**\n",
    "- 训练模型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 导入所需的Python库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import load_mnist\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 加载数据并可视化\n",
    "\n",
    "在这个实验中用到的数据集是MNIST。MNIST数据集是一个手写体数据集，包含训练集**60000**张图片和测试集**10000**张图片，其中每张图片是一张**28×28**的灰度图片，图片内容为一个**0-9**表示的手写数字。\n",
    "一张灰度图片读取后是二维张量的，但是我们使用的神经网络中的全连接层的输入是一维张量。所以我们需要将一张二维灰度图片转换成一个一维张量（即向量），如图1：\n",
    "\n",
    "<div align=center> \n",
    "<img src=\"./images/2d_to_1d.jpg\" width=\"300\" height=\"300\" alt=\"2d_to_1d\" >\n",
    "<br>\n",
    "图1. 展开二维图片成向量\n",
    "</div>\n",
    "\n",
    "在此之后每一个样本都是一个长度为**784**（28*28）的向量。 \n",
    "\n",
    "手写数字识别任务是一个多分类任务，共包含有10类，分别用0-9表示，为了用一个向量表示一个类别，通常使用**one-hot**编码，比如在手写数字识别中，类别0和1对应的one-hot向量分别为： \n",
    "$$\\mathrm{one-hot}(0)=\\begin{bmatrix}1 \\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ \\end{bmatrix},\\mathrm{one-hot}(1)=\\begin{bmatrix}0 \\\\ 1\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ \\end{bmatrix}$$ \n",
    "\n",
    "<div align=center>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 加载数据\n",
    "在训练集中，我们需要把样本的标记$Y$转化为one-hot向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载训练集或测试集\n",
    "path = './MNIST Data' #数据集文件所在目录\n",
    "# 加载训练集合测试集\n",
    "# 设置normalization为True，将数据缩放到[0,1]之间\n",
    "# 设置one_hot_label为True，将标签转化为one_hot向量\n",
    "(x_train, y_train), (x_test, y_test) = load_mnist(path, normalize=True, one_hot_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X_train is: (60000, 784)\n",
      "The shape of Y_train is: (60000, 10)\n",
      "The shape of X_test is: (10000, 784)\n",
      "The shape of Y_test is: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print('The shape of X_train is:',x_train.shape)\n",
    "print('The shape of Y_train is:',y_train.shape)\n",
    "print('The shape of X_test is:',x_test.shape)\n",
    "print('The shape of Y_test is:',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 可视化数据集\n",
    "下面在训练集中选取几个图片看一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one hot 标签： [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "对应的实际标签： 0 4 1 9\n"
     ]
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot(141)\n",
    "ax1.imshow(x_train[1,:].reshape(28, 28), cmap='Greys')\n",
    "ax2 = fig.add_subplot(142)\n",
    "ax2.imshow(x_train[2,:].reshape(28,28), cmap='Greys')\n",
    "ax3 = fig.add_subplot(143)\n",
    "ax3.imshow(x_train[3,:].reshape(28,28), cmap='Greys')\n",
    "ax4 = fig.add_subplot(144)\n",
    "ax4.imshow(x_train[4,:].reshape(28,28), cmap='Greys')\n",
    "plt.show()\n",
    "print('one hot 标签：',y_train[1,:],y_train[2,:],y_train[3,:],y_train[4,:])\n",
    "print('对应的实际标签：',np.argmax(y_train[1,:]),np.argmax(y_train[2,:]),np.argmax(y_train[3,:]),np.argmax(y_train[4,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 全连接神经网络\n",
    "\n",
    "接下来，搭建一个**三层全连接神经网络**。网络结构见图二：\n",
    "\n",
    "<div align=center> \n",
    "<img src=\"./images/network_architecture.jpg\" width=\"500\" height=\"250\" alt=\"network_architecture\" align=center>\n",
    "<br>\n",
    "图2：三层全连接神经网络结构\n",
    "</div>\n",
    "\n",
    "对公式所用符号的说明：  \n",
    "一般计算神经网络层数不算输入层，所以图2中为3层神经网络。用右上角的方括号表示相应的层，所有第1层的权重$W$为$W^{[1]}$，第1层的偏置项$b$为$b^{[1]}$(图中未标出)，第1层的激活值$A$为$A^{[1]}$。  \n",
    "\n",
    "前两层的激活函数使用LeakyRelu，最后一层使用Softmax进行分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 初始化网络参数\n",
    "\n",
    "我们规定第1层的神经元个数为300，第二层个数为300，最后一层为10。输入向量$X$的维度为784，那么整个网络对应的参数也就可以确定了。  \n",
    "$W^{[1]}$的shape为$(784,300)$，$b^{[1]}$的shape为$(300,)$  \n",
    "$W^{[2]}$的shape为$(300,300)$，$b^{[2]}$的shape为$(300,)$  \n",
    "$W^{[3]}$的shape为$(300,10)$，$b^{[3]}$的shape为$(10,)$  \n",
    "这里**使用随机正态分布乘上比例因子0.01来初始化$W$， 把$b$都初始化为0**.  \n",
    "**Hint**: 使用`np.random.randn()`,`np.zeros()`  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(input_size, hidden_size, output_size, weight_init_std):\n",
    "    \"\"\"\n",
    "    @param input_size:输入向量维度\n",
    "    @param hidden_size:中间神经元个数\n",
    "    @param output_size:输出层神经元个数\n",
    "    @param weight_init_sta:比例因子\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    params = {}\n",
    "\n",
    "    params['W1'] = np.random.randn(input_size,hidden_size) * weight_init_std\n",
    "    params['b1'] = np.zeros((hidden_size,)) #请参考样例完成代码\n",
    "    ### START CODE HERE ### \n",
    "    params['W2'] = np.random.randn(hidden_size,hidden_size) * weight_init_std\n",
    "    params['b2'] = np.zeros((hidden_size,))\n",
    "    params['W3'] = np.random.randn(hidden_size,output_size) * weight_init_std\n",
    "    params['b3'] = np.zeros((output_size,))\n",
    "    \n",
    "    ### END CODE HERE ### \n",
    "\n",
    "    print(\"W1's shape:\",params['W1'].shape)\n",
    "    print(\"b1's shape:\",params['b1'].shape)\n",
    "    print(\"W2's shape:\",params['W2'].shape)\n",
    "    print(\"b2's shape:\",params['b2'].shape)\n",
    "    print(\"W3's shape:\",params['W3'].shape)\n",
    "    print(\"b3's shape:\",params['b3'].shape) #请在调用该函数的地方观察该神经网络各个参数的shape，是否符合预期\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 定义神经网络的每一层\n",
    "我们将用单独的类来实现各种神经网络层：\n",
    "- LeakyRelu\n",
    "- Affine\n",
    "- Softmax-with-loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 LeakyRelu层\n",
    "\n",
    "激活函数LeakyRelu的表达式为：\n",
    "$$\n",
    "\\left\\{\\begin{matrix}\n",
    "x & (x>0)\\\\ \n",
    "\\alpha x & (x \\leq 0)\n",
    "\\end{matrix}\\right.\n",
    "$$\n",
    "可以通过上式求出$y$关于$x$的导数：\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x} = \\left\\{\\begin{matrix}\n",
    "1 & (x>0)\\\\ \n",
    "\\alpha & (x \\leq 0)\n",
    "\\end{matrix}\\right.\n",
    "$$\n",
    "\n",
    "如果正向传播时的输入$x$大于0，则反向传播会将下游的值原封不动地传给上游。反过来，如果正向传播时的$x$小于等于0，则反向传播中传给上游的信号将乘上一个很小的常数，保证neuron依然起作用。\n",
    "\n",
    "<div align=center>\n",
    "<img src=\"./images/LeakyReLu.png\" width=\"350\" height=\"250\" alt=\"ReLU\" align=center>\n",
    "<br>\n",
    "图3. LeakyRelu\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class LeakyRelu:\n",
    "    def __init__(self):\n",
    "        self.mask = None \n",
    "        self.alpha = 0.1\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0) #mask表示选择出x的值中小于等于0的部分内容\n",
    "        out = x.copy()\n",
    "        ### START CODE HERE ###  #请参考LeakyRelu表达式实现前向传播过程\n",
    "        out[self.mask] = self.alpha*out[self.mask]\n",
    "        ### END CODE HERE ### \n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        ### START CODE HERE ###  #请参考LeakyRelu表达式y关于x的导数公式实现反向传播过程\n",
    "        dout[self.mask] = self.alpha*dout[self.mask]\n",
    "        ### END CODE HERE ### \n",
    "        dx = dout\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.  , -0.05],\n",
       "        [-0.2 ,  3.  ]]),\n",
       " array([[ 1.  , -0.05],\n",
       "        [-0.2 ,  3.  ]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leakyRelu = LeakyRelu()\n",
    "x = np.array( [[1.0, -0.5], [-2.0, 3.0]] )\n",
    "leakyRelu.forward(x), leakyRelu.backward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Affine层\n",
    "\n",
    "神经网络的正向传播中，为了计算加权信号的总和，使用了矩阵的乘积运算，回顾一下公式：\n",
    "$$\n",
    "Y = XW + B\n",
    "$$\n",
    "即`Y = np.dot(X, W) + B`。假设X，W，B的shape分别为(2,)、(2,3)、(3,)如图4，现在将这里进行的求矩阵的乘积与偏置的和的运算用计算图表示出来。将乘积运算用“dot”节点表示的话，则np.dot(X, W) + B的运算如图5：\n",
    "<div align=center>\n",
    "<img src=\"./images/Affine1.png\"><br>\n",
    "图4. 单向量正向传播\n",
    "</div>\n",
    "<div align=center>\n",
    "<img src=\"./images/Affine2.png\"><br>\n",
    "图5. Affine\n",
    "</div>\n",
    "\n",
    "以矩阵为对象的反向传播，按矩阵的各个元素进行计算时，步骤和以标量为对象的计算图相同。用公式表示：\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y} \\cdot W^T\n",
    "$$\n",
    "  \n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = X^T \\cdot \\frac{\\partial L}{\\partial Y}\n",
    "$$\n",
    "根据这个式子我们可以得到一个向量反向传播的计算图（见图6）。\n",
    "<div align=center>\n",
    "<img src=\"./images/Affine3.png\"><br>\n",
    "图6. 单向量反向传播\n",
    "</div>\n",
    "\n",
    "\n",
    "现在我们考虑N个数据一起进行正向传播（见图7），与刚刚不同的是，现在输入X的shape是(N, 2)。之后就和前面一样，在计算图上进行矩阵计算。反向传播时，需要注意矩阵的shape，推导出$\\frac{\\partial L}{\\partial X}$ 和 $\\frac{\\partial L}{\\partial W}$的过程与单个向量类似。\n",
    "\n",
    "<div align=center>\n",
    "<img src=\"./images/Affine4.png\"><br>\n",
    "图7. N个向量反向传播\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     1,
     20
    ]
   },
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        # 权重和偏置参数的导数\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "        ### START CODE HERE ### \n",
    "        out = np.dot(x,self.W)+self.b\n",
    "        ### END CODE HERE ### \n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        ### START CODE HERE ### \n",
    "        self.dW = np.dot(self.x.T,dout)\n",
    "        self.db = np.sum(dout,axis=0)\n",
    "        ### END CODE HERE ### \n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)  # 还原输入数据的形状（对应张量）\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9.0, array([[3., 3.]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.ones([2, 3])\n",
    "b = np.ones(3)\n",
    "affine = Affine(w, b)\n",
    "x = np.ones([1, 2])\n",
    "# 假设 l = sum(y)\n",
    "l = np.sum(affine.forward(x))\n",
    "dout = np.ones([1,3])\n",
    "l, affine.backward(dout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Softmax-with-Loss层\n",
    "\n",
    "softmax函数会将输入值正规化后输出，如图所示：\n",
    "<div align=center>\n",
    "<img src=\"./images/softmax.png\">\n",
    "<br>\n",
    "图8. Softmax层\n",
    "</div>\n",
    "\n",
    "用$x_i$表示向量$x$的第$i$个分量。具体公式为\n",
    "$$softmax(x_i)=\\frac{e^{x_i}}{\\sum_{j=1}^{C}{e^{x_j}}}$$ \n",
    "\n",
    "\n",
    "当最后一层为softmax时，使用的损失函数一般为交叉熵函数（$C$是类别的数量，在本次实验中即为10）：  \n",
    "$$L(\\hat{y},y)=-\\sum_{j=1}^{C}{y_jlog\\hat{y_j}}$$  \n",
    "成本函数为（$m$是mini-batch的大小）：  \n",
    "$$J(W^{[1]},b^{[1]},...)=\\frac{1}{m}\\sum_{i=1}^{m}{L(\\hat{y}^{(i)},y^{(i)})}$$  \n",
    "**mini-batch**：训练集共有60000个图片，我们不能把$(60000,784)$的张量直接放入神经网络计算。因此，每次正向传播从中选取m个图像来进行前向传播，所以输入的张量大小为$(m,784)$.\n",
    "\n",
    "**Hint**：最后的Loss是个标量 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x = x.T\n",
    "    x = x - np.max(x, axis=0)\n",
    "    y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "    return y.T \n",
    "\n",
    "def cross_entropy_error(pred, y):\n",
    "    if pred.ndim == 1:\n",
    "        y = y.reshape(1, y.size)\n",
    "        pred = pred.reshape(1, pred.size)\n",
    "        \n",
    "    # 监督数据是one-hot-vector的情况下，转换为正确解标签的索引\n",
    "    if y.size == pred.size:\n",
    "        y = y.argmax(axis=1)\n",
    "             \n",
    "    batch_size = pred.shape[0]\n",
    "    \n",
    "    res = None\n",
    "\n",
    "    res = -np.sum(np.log(pred[:, y] + 1e-7)) / batch_size\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现Softmax 层（已完成）\n",
    "\n",
    "考虑到这里也包含作为损失函数的交叉熵误差（cross entropy error），所以称为“Softmax-with-Loss 层”。Softmax-with-Loss 层（Softmax函数和交叉熵误差）的计算图如图9所示。\n",
    "\n",
    "<div align=center>\n",
    "<img src=\"./images/softmax-cross.png\" align=center>\n",
    "<br>\n",
    "图9. Softmax-with-Loss 层\n",
    "</div>\n",
    "可以看到，Softmax-with-Loss 层有些复杂。这里只给出了最终结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.pred = None # softmax的输出\n",
    "        self.y = None # 监督数据\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        self.y = y\n",
    "        self.pred = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.pred, self.y)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.y.shape[0]\n",
    "        if self.y.size == self.pred.size: # 监督数据是one-hot-vector的情况\n",
    "            dx = (self.pred - self.y) / batch_size\n",
    "        else:\n",
    "            dx = self.pred.copy()\n",
    "            dx[np.arange(batch_size), self.y] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 搭积木\n",
    "现在我们把之前的每一层组合在一起就能搭成我们自己的三层神经网络了。\n",
    "神经网络学习的步骤大致有5步：\n",
    "1. 初始化权重\n",
    "2. 随机选择一部分数据\n",
    "3. 计算梯度\n",
    "4. 更新参数\n",
    "5. 重复步骤2,3,4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0,
     1,
     18,
     27,
     35,
     48
    ]
   },
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        \n",
    "        # 初始化权重\n",
    "        self.params = initialize_parameters(input_size, hidden_size, output_size, weight_init_std)\n",
    "        # 记录训练次数 adam里要用\n",
    "        self.t = 0\n",
    "\n",
    "        # 生成层\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['LeakyRelu1'] = LeakyRelu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['LeakyRelu2'] = LeakyRelu()\n",
    "        self.layers['Affine3'] = Affine(self.params['W3'], self.params['b3'])\n",
    "    \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # 前向传播\n",
    "        pred = x.copy()\n",
    "        for layer in self.layers.values():\n",
    "            # 通过forward函数完成前向传播\n",
    "            ### START CODE HERE ###\n",
    "            pred = layer.forward(pred)\n",
    "            ### END CODE HERE ###\n",
    "            \n",
    "        return pred\n",
    "        \n",
    "    def loss(self, x, y):\n",
    "        # 计算交叉熵损失\n",
    "        ### START CODE HERE ### \n",
    "        pred = self.predict(x) #计算关于x的预测结果\n",
    "        loss = self.lastLayer.forward(pred,y) #使用SoftmaxWithLoss层计算预测结果和y之间的交叉熵损失\n",
    "        ### END CODE HERE ### \n",
    "        return loss\n",
    "    \n",
    "    def accuracy(self, x, y):\n",
    "        # 输入数据x和标签y，输出当前神经网络的预测准确率\n",
    "        accuracy = None\n",
    "        pred = self.predict(x)\n",
    "        pred = np.argmax(pred, axis=1)\n",
    "        if y.ndim != 1:\n",
    "            y = np.argmax(y, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(pred == y) / float(x.shape[0])\n",
    "\n",
    "        return accuracy\n",
    "        \n",
    "    def gradient(self, x, y):\n",
    "        # 前向传播\n",
    "        self.loss(x, y)\n",
    "\n",
    "        # 反向传播\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 设定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine3'].dW, self.layers['Affine3'].db\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 进行训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1 定义参数更新函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def update_parameters(network, grads, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    使用梯度下降法更新network的参数\n",
    "    \"\"\"\n",
    "\n",
    "    #在这里我们给出了最基础的梯度下降法更新网络参数的实现代码，请同学们参考并完成其他优化算法的代码\n",
    "    \n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        network.params[key] -= learning_rate * grads[key]  #在network现在的参数基础上减去学习率*梯度\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.2 定义训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def train_network(network, update_params_method, iters_num, train_size, batch_size, learning_rate):\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "    test_acc_list = []\n",
    "\n",
    "    iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "    for i in range(iters_num):\n",
    "        batch_mask = np.random.choice(train_size, batch_size)\n",
    "        x_batch = x_train[batch_mask]\n",
    "        t_batch = y_train[batch_mask]\n",
    "        network.t += 1\n",
    "\n",
    "        # 计算梯度\n",
    "        grad = network.gradient(x_batch, t_batch)\n",
    "\n",
    "        # 更新梯度\n",
    "        update_params_method(network, grad, learning_rate)\n",
    "\n",
    "        loss = network.loss(x_batch, t_batch)\n",
    "        train_loss_list.append(loss)\n",
    "\n",
    "        if i % iter_per_epoch == 0:\n",
    "            train_acc = network.accuracy(x_train, y_train)\n",
    "            test_acc = network.accuracy(x_test, y_test)\n",
    "            train_acc_list.append(train_acc)\n",
    "            test_acc_list.append(test_acc)\n",
    "            print(\"Train acc:{:<.6f}\\tTest acc:{:<.6f}\".format(train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入数据\n",
    "(x_train, y_train), (x_test, y_test) = load_mnist(path, normalize=True, one_hot_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1's shape: (784, 300)\n",
      "b1's shape: (300,)\n",
      "W2's shape: (300, 300)\n",
      "b2's shape: (300,)\n",
      "W3's shape: (300, 10)\n",
      "b3's shape: (10,)\n"
     ]
    }
   ],
   "source": [
    "# 定义神经网络\n",
    "network = TwoLayerNet(input_size=784, hidden_size=300, output_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc:0.085400\tTest acc:0.084700\n",
      "Train acc:0.882817\tTest acc:0.887600\n",
      "Train acc:0.921217\tTest acc:0.918800\n",
      "Train acc:0.941400\tTest acc:0.940300\n",
      "Train acc:0.954433\tTest acc:0.953100\n",
      "Train acc:0.966600\tTest acc:0.961500\n",
      "Train acc:0.972000\tTest acc:0.965700\n",
      "Train acc:0.975467\tTest acc:0.967000\n",
      "Train acc:0.978200\tTest acc:0.970800\n",
      "Train acc:0.982183\tTest acc:0.972900\n",
      "Train acc:0.985733\tTest acc:0.977900\n",
      "Train acc:0.986983\tTest acc:0.976200\n",
      "Train acc:0.986350\tTest acc:0.974700\n",
      "Train acc:0.989633\tTest acc:0.977500\n",
      "Train acc:0.991583\tTest acc:0.978200\n",
      "Train acc:0.992133\tTest acc:0.978600\n",
      "Train acc:0.990667\tTest acc:0.977800\n"
     ]
    }
   ],
   "source": [
    "iters_num = 10000 #迭代次数\n",
    "train_size = x_train.shape[0] #训练集的样本数量\n",
    "batch_size = 100 #batch大小\n",
    "learning_rate = 0.1 #学习率\n",
    "train_network(network, update_parameters, iters_num, train_size, batch_size, learning_rate) #开始训练网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 梯度下降优化算法\n",
    "下面回顾几种上课讲过的优化算法，注意它们之间的差异与联系。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.1AdaGrad  \n",
    "\n",
    "当$J(\\theta)$收敛到最低值附近时，因为步长$\\alpha$大小固定的原因，$J(\\theta)$会在最低值附近徘徊，而不能到达最低值。因此，AdaGrad的想法是随着迭代次数的增加降低学习率$\\alpha$，学习率$\\alpha$衰减的方式是\n",
    "$$\n",
    "\\alpha^t = \\frac{\\alpha}{\\sqrt{t+1}}\n",
    "$$\n",
    "其中t表示第t次迭代。\n",
    "\n",
    "如果梯度数值小，$J(\\theta)$的移动步长小，$J(\\theta)$在坡度平缓的区域内下降速度会变慢。AdaGrad使用均方根来加快$J(\\theta)$在平缓区域的下降速度。均方根的表示为\n",
    "$$\n",
    "\\sigma^t = \\sqrt{\\frac{1}{t+1} \\sum_{i=0}^{t}(g^i)2}\n",
    "$$\n",
    "其中$g^i$表示历史的梯度值。AdaGrad 的更新参数公式是\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\theta^{t+1} & := \\theta^t - \\frac{\\alpha^t}{\\sigma^t} g^t \\\\\n",
    "& := \\theta^t - \\frac{\\alpha}{\\sqrt{\\sum_{i=0}^t (g^i)^2}} g^t\n",
    "\\end{aligned}\n",
    "$$\n",
    "在坡度平缓的区域，均方根的数值小，梯度除以一个数值小的数会变大，从而加大了$J(\\theta)$移动步长，也因此加快梯度下降速度。但是，AdaGrad的缺点是，随着迭代次数的增大，均方根会越来越大，梯度趋近于0，导致训练提前停止。为了防止分母为0，我们给分母加上一个小数值$\\epsilon =10^{-7}$。\n",
    "$$\n",
    "\\theta^{t+1} := \\theta^t - \\frac{\\alpha}{\\sqrt{\\sum_{i=0}^t (g^i)^2} + \\epsilon} g^t\n",
    "$$\n",
    "\n",
    "我们可以看到分母里会计算所有历史梯度值的平方和，所以在实现的时候不用保存所有的历史梯度值，只需要保存一个纪录所有历史梯度平方和的值即可。每个参数的历史梯度和初始值为0。   \n",
    "在代码实现中，我们使用`epsilon`代表ε  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def initialize_grads_squared(network):\n",
    "    \"\"\"\n",
    "    初始化历史梯度和\n",
    "    \"\"\"\n",
    "    grads_squared = {}\n",
    "    \n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        \n",
    "        grads_squared[key] = np.zeros(network.params[key].shape)\n",
    "        \n",
    "    return grads_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1's shape: (784, 300)\n",
      "b1's shape: (300,)\n",
      "W2's shape: (300, 300)\n",
      "b2's shape: (300,)\n",
      "W3's shape: (300, 10)\n",
      "b3's shape: (10,)\n"
     ]
    }
   ],
   "source": [
    "network = TwoLayerNet(input_size=784, hidden_size=300, output_size=10)\n",
    "grads_squared = initialize_grads_squared(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def update_parameters_with_adagrad(network, grads, learning_rate=0.001, epsilon = 1e-7):\n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        ### START CODE HERE ### \n",
    "        \n",
    "        #计算历史梯度平方和\n",
    "        grads_squared[key] += grads[key]*grads[key]\n",
    "        network.params[key] -= learning_rate*grads[key]/(np.sqrt(grads_squared[key])+epsilon)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc:0.224533\tTest acc:0.221600\n",
      "Train acc:0.868767\tTest acc:0.870600\n",
      "Train acc:0.886033\tTest acc:0.887800\n",
      "Train acc:0.894200\tTest acc:0.895500\n",
      "Train acc:0.899167\tTest acc:0.899200\n",
      "Train acc:0.902833\tTest acc:0.902300\n",
      "Train acc:0.905533\tTest acc:0.905000\n",
      "Train acc:0.908300\tTest acc:0.907900\n",
      "Train acc:0.909200\tTest acc:0.910200\n",
      "Train acc:0.912150\tTest acc:0.911400\n",
      "Train acc:0.914283\tTest acc:0.913000\n",
      "Train acc:0.916233\tTest acc:0.915000\n",
      "Train acc:0.916717\tTest acc:0.916500\n",
      "Train acc:0.918017\tTest acc:0.915700\n",
      "Train acc:0.919483\tTest acc:0.918000\n",
      "Train acc:0.920233\tTest acc:0.919800\n",
      "Train acc:0.921217\tTest acc:0.920200\n"
     ]
    }
   ],
   "source": [
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "train_network(network, update_parameters_with_adagrad, iters_num, train_size, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.2 RMSprop  \n",
    "从AdaGrad算法的公式可看出，所有时刻的梯度都对当前的参数更新有影响。如果早先的梯度并不是一个正确的方向，那么这些糟糕的梯度还是会影响到当前的参数更新。因此，RMSprop相当于就是只记录当前时刻前的某一段历史梯度和而不是所有历史梯度和。  \n",
    "RMSprop算法的公式如下：  \n",
    "$$\n",
    " u^0 = 0 \\\\\n",
    " u^{t+1} = \\rho u^t + (1-\\rho) [\\nabla J(\\theta ^t)]^2 \\\\ \n",
    " \\theta^{t+1} = \\theta^t - \\frac{\\alpha}{\\sqrt{u^{t+1}}+\\epsilon}\\nabla J(\\theta ^t) \n",
    "$$\n",
    "这里$\\rho$是超参数，一般设为0.999，也不会调它。$\\epsilon$是防止分母为0。另外值得注意的是，因为要整合这几个算法在一起，而Adam算法又融合了各种算法，所以，关于优化算法的超参数的命名与Adam里保持一致，公式里的$\\rho$用下面参数`beta`代替。这些算法几乎都要保存一些变量，它们的初始化基本与AdaGrad初始化的方法一致，所以这部分初始化的代码就不重复了。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def update_parameters_with_rmsprop(network, grads, learning_rate=0.001, epsilon = 1e-7, beta=0.999):\n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        ### START CODE HERE ### \n",
    "        \n",
    "        #公式里的u就是这里的 grads_squared         \n",
    "        grads_squared[key] = beta*grads_squared[key]+(1-beta)*grads[key]*grads[key]\n",
    "        network.params[key] -= learning_rate/(np.sqrt(grads_squared[key])+epsilon)*grads[key]\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1's shape: (784, 300)\n",
      "b1's shape: (300,)\n",
      "W2's shape: (300, 300)\n",
      "b2's shape: (300,)\n",
      "W3's shape: (300, 10)\n",
      "b3's shape: (10,)\n"
     ]
    }
   ],
   "source": [
    "network = TwoLayerNet(input_size=784, hidden_size=300, output_size=10)\n",
    "grads_squared = initialize_grads_squared(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc:0.154500\tTest acc:0.145600\n",
      "Train acc:0.964933\tTest acc:0.959300\n",
      "Train acc:0.975267\tTest acc:0.968700\n",
      "Train acc:0.984133\tTest acc:0.973400\n",
      "Train acc:0.987083\tTest acc:0.975700\n",
      "Train acc:0.988967\tTest acc:0.975400\n",
      "Train acc:0.992717\tTest acc:0.977400\n",
      "Train acc:0.994050\tTest acc:0.977900\n",
      "Train acc:0.993700\tTest acc:0.978300\n",
      "Train acc:0.993617\tTest acc:0.976400\n",
      "Train acc:0.993650\tTest acc:0.976300\n",
      "Train acc:0.993333\tTest acc:0.976200\n",
      "Train acc:0.995350\tTest acc:0.980400\n",
      "Train acc:0.996850\tTest acc:0.979400\n",
      "Train acc:0.995117\tTest acc:0.977200\n",
      "Train acc:0.994083\tTest acc:0.975900\n",
      "Train acc:0.998200\tTest acc:0.980000\n"
     ]
    }
   ],
   "source": [
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "train_network(network, update_parameters_with_rmsprop, iters_num, train_size, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.3 Momentum\n",
    "动量梯度下降（Gradient Descent with Momentum）基本思想就是计算梯度的指数加权平均数，并利用该指数加权平均数更新权重。具体过程为：\n",
    "$$\n",
    "v^0 = 0 \\\\\n",
    "v^{t+1}  = \\rho v^t +\\alpha \\nabla J(\\theta ^t) \\\\\n",
    "\\theta^{t+1} = \\theta ^t - v^{t+1}\n",
    "$$\n",
    "\n",
    "这里的$\\rho$一般取0.9。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def initialize_velocity(network):\n",
    "    v = {}\n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        v[key] = np.zeros((network.params[key]).shape) \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1's shape: (784, 300)\n",
      "b1's shape: (300,)\n",
      "W2's shape: (300, 300)\n",
      "b2's shape: (300,)\n",
      "W3's shape: (300, 10)\n",
      "b3's shape: (10,)\n"
     ]
    }
   ],
   "source": [
    "network = TwoLayerNet(input_size=784, hidden_size=300, output_size=10)\n",
    "v = initialize_velocity(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def update_parameters_with_momentum(network, grads, learning_rate=0.001, beta=0.9):\n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        ### START CODE HERE ### \n",
    "        \n",
    "        #公式里的u就是这里的 grads_squared         \n",
    "        v[key] = beta*v[key]+learning_rate*grads[key]\n",
    "        network.params[key] -= v[key]\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc:0.085050\tTest acc:0.084000\n",
      "Train acc:0.112367\tTest acc:0.113500\n",
      "Train acc:0.282417\tTest acc:0.284400\n",
      "Train acc:0.437133\tTest acc:0.447100\n",
      "Train acc:0.702383\tTest acc:0.715900\n",
      "Train acc:0.789133\tTest acc:0.797000\n",
      "Train acc:0.828717\tTest acc:0.835100\n",
      "Train acc:0.852583\tTest acc:0.856400\n",
      "Train acc:0.869817\tTest acc:0.873100\n",
      "Train acc:0.882033\tTest acc:0.884900\n",
      "Train acc:0.890717\tTest acc:0.890700\n",
      "Train acc:0.896317\tTest acc:0.894500\n",
      "Train acc:0.899783\tTest acc:0.899300\n",
      "Train acc:0.904583\tTest acc:0.902500\n",
      "Train acc:0.908617\tTest acc:0.906200\n",
      "Train acc:0.911867\tTest acc:0.909400\n",
      "Train acc:0.914083\tTest acc:0.912900\n"
     ]
    }
   ],
   "source": [
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "train_network(network, update_parameters_with_momentum, iters_num, train_size, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.4 Nesterov Momentum  \n",
    "Nesterov Momentum算法与Momentum不同的是在于，它会提前计算一个在速度作用后的梯度。具体算法如下：\n",
    "$$\n",
    "v^{t+1} = \\rho v^t + \\alpha \\nabla J(\\theta ^t - \\rho v^t) \\\\\n",
    "\\theta^{t+1} = \\theta ^t - v^{t+1}\n",
    "$$\n",
    "但是在实现的时候，我们是不会算一次$J(\\theta ^t)$再算一次$\\nabla J(\\theta ^t - \\rho v^t)$的。具体编程实现时上式等价于下式：\n",
    "$$\n",
    " v^{t+1} = \\rho v^t + \\alpha \\nabla J(\\theta ^t) \\\\\n",
    " \\theta^{t+1} = \\theta ^t - \\rho v^{t+1} - \\alpha \\nabla J(\\theta ^t)\n",
    "$$\n",
    "这里的$\\rho$一般取0.9。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_nesterov_momentum(network, grads, learning_rate=0.001, beta=0.9):\n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        ### START CODE HERE ### \n",
    "                \n",
    "        v[key] = beta*v[key]+learning_rate*grads[key]\n",
    "        network.params[key] -= beta*v[key]+learning_rate*grads[key]\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1's shape: (784, 300)\n",
      "b1's shape: (300,)\n",
      "W2's shape: (300, 300)\n",
      "b2's shape: (300,)\n",
      "W3's shape: (300, 10)\n",
      "b3's shape: (10,)\n"
     ]
    }
   ],
   "source": [
    "network = TwoLayerNet(input_size=784, hidden_size=300, output_size=10)\n",
    "v = initialize_velocity(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc:0.084783\tTest acc:0.084100\n",
      "Train acc:0.112367\tTest acc:0.113500\n",
      "Train acc:0.283333\tTest acc:0.285600\n",
      "Train acc:0.440450\tTest acc:0.450000\n",
      "Train acc:0.703683\tTest acc:0.717300\n",
      "Train acc:0.789483\tTest acc:0.796600\n",
      "Train acc:0.829000\tTest acc:0.835400\n",
      "Train acc:0.853417\tTest acc:0.856900\n",
      "Train acc:0.869817\tTest acc:0.873300\n",
      "Train acc:0.882167\tTest acc:0.884300\n",
      "Train acc:0.890650\tTest acc:0.890700\n",
      "Train acc:0.896400\tTest acc:0.894400\n",
      "Train acc:0.899950\tTest acc:0.899100\n",
      "Train acc:0.904733\tTest acc:0.902300\n",
      "Train acc:0.908700\tTest acc:0.906000\n",
      "Train acc:0.911867\tTest acc:0.909300\n",
      "Train acc:0.914117\tTest acc:0.912800\n"
     ]
    }
   ],
   "source": [
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "train_network(network, update_parameters_with_nesterov_momentum, iters_num, train_size, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.5 Adam\n",
    "Adam优化算法（Adaptive Moment Estimation）是将Momentum和RMSprop结合在一起的算法，具体过程如下\n",
    "$$\n",
    "u^0 = 0 \\\\\n",
    "v^0 = 0 \\\\\n",
    "u^{t+1}  = \\rho_2 u^t +(1-\\rho_2) [\\nabla J(\\theta ^t)]^2 \\\\\n",
    "v^{t+1}  = \\rho_1 v^t + (1-\\rho_1)\\nabla J(\\theta ^t) \\\\\n",
    "\\theta^{t+1} = \\theta ^t - \\frac{\\alpha}{\\sqrt{u^{t+1}}+\\epsilon}v^{t+1}\n",
    "$$\n",
    "从上式可以看到，在最开始更新时，$u^{t},v^{t}$都是很小的。所以需要对早期的更新进行一个bias correction。完整公式如下\n",
    "$$\n",
    "u^0 = 0 \\\\\n",
    "v^0 = 0 \\\\\n",
    "u^{t+1}  = \\rho_2 u^t +(1-\\rho_2) [\\nabla J(\\theta ^t)]^2 \\\\\n",
    "u^{t+1}_{corrected} = \\frac{u^{t+1}}{1-\\rho_2^t} \\\\\n",
    "v^{t+1}  = \\rho_1 v^t + (1-\\rho_1)\\nabla J(\\theta ^t) \\\\\n",
    "v^{t+1}_{corrected} = \\frac{v^{t+1}}{1-\\rho_1^t} \\\\\n",
    "\\theta^{t+1} = \\theta ^t - \\frac{\\alpha}{\\sqrt{u^{t+1}_{corrected}}+\\epsilon}v^{t+1}_{corrected}\n",
    "$$\n",
    "\n",
    "其中，一般设$\\rho_1=0.9,\\rho_2=0.999$.$\\epsilon$也是防止分母过小或等于0.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def initialize_adam(network) :\n",
    "    v = {}\n",
    "    u = {}\n",
    "\n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        v[key] = np.zeros(np.shape(network.params[key]))\n",
    "        u[key] = np.zeros(np.shape(network.params[key]))\n",
    "            \n",
    "    return v, u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1's shape: (784, 300)\n",
      "b1's shape: (300,)\n",
      "W2's shape: (300, 300)\n",
      "b2's shape: (300,)\n",
      "W3's shape: (300, 10)\n",
      "b3's shape: (10,)\n"
     ]
    }
   ],
   "source": [
    "network = TwoLayerNet(input_size=784, hidden_size=300, output_size=10)\n",
    "v, u = initialize_adam(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def update_parameters_with_adam(network, grads, learning_rate=0.001, epsilon=1e-7, beta1=0.9, beta2=0.999):\n",
    "    v_corrected = {}\n",
    "    u_corrected = {} \n",
    "    t = network.t #当前迭代次数\n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        ### START CODE HERE ### \n",
    "                \n",
    "        v[key] = beta1*v[key]+(1-beta1)*grads[key]\n",
    "        v_corrected[key] = v[key]/(1-beta1**t)\n",
    "        \n",
    "        u[key] = beta2*u[key]+(1-beta2)*grads[key]*grads[key]\n",
    "        u_corrected[key] = u[key]/(1-beta2**t)\n",
    "        \n",
    "        network.params[key] -= learning_rate/(np.sqrt(u_corrected[key])+epsilon)*v_corrected[key]\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc:0.143983\tTest acc:0.144800\n",
      "Train acc:0.958583\tTest acc:0.954000\n",
      "Train acc:0.971600\tTest acc:0.966900\n",
      "Train acc:0.978850\tTest acc:0.967800\n",
      "Train acc:0.983017\tTest acc:0.972800\n",
      "Train acc:0.985367\tTest acc:0.972400\n",
      "Train acc:0.986133\tTest acc:0.974500\n",
      "Train acc:0.991683\tTest acc:0.978100\n",
      "Train acc:0.986633\tTest acc:0.971500\n",
      "Train acc:0.995467\tTest acc:0.981400\n",
      "Train acc:0.993783\tTest acc:0.980900\n",
      "Train acc:0.994167\tTest acc:0.980400\n",
      "Train acc:0.994417\tTest acc:0.978400\n",
      "Train acc:0.995717\tTest acc:0.979500\n",
      "Train acc:0.997383\tTest acc:0.980900\n",
      "Train acc:0.996250\tTest acc:0.978900\n",
      "Train acc:0.996833\tTest acc:0.980800\n"
     ]
    }
   ],
   "source": [
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "train_network(network, update_parameters_with_adam, iters_num, train_size, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.6 AdaBelief\n",
    "Adam优化算法中，梯度更新的方向是 $\\frac{v_{t+1}}{\\sqrt{u_{t+1}}}$, 其中 $u_{t+1}$ 是 $[\\nabla J(\\theta ^t)]^2$ 的指数移动平均(exponential moving average,EMA)。\n",
    "\n",
    "AdaBelief优化算法将梯度方向改为了 $\\frac{v_{t+1}}{\\sqrt{s_{t+1}}}$, 其中 $s_{t+1}$ 是 $[\\nabla J(\\theta ^t)-v_{t}]^2$ 的指数移动平均，AdaBelief的算法过程具体如下：\n",
    "$$\n",
    "s^0 = 0 \\\\\n",
    "v^0 = 0 \\\\\n",
    "v^{t+1}  = \\rho_1 v^t + (1-\\rho_1)\\nabla J(\\theta ^t) \\\\\n",
    "s^{t+1}  = \\rho_2 s^t +(1-\\rho_2) [\\nabla J(\\theta ^t)-v_{t+1}]^2 \\\\\n",
    "\\theta^{t+1} = \\theta ^t - \\frac{\\alpha}{\\sqrt{s^{t+1}}+\\epsilon}v^{t+1}\n",
    "$$\n",
    "\n",
    "与Adam优化算法同理，从上式可以看到，AdaBelief优化算法在最开始更新时，$s^{t},v^{t}$都是很小的。所以需要对早期的更新进行一个bias correction。完整公式如下\n",
    "$$\n",
    "s^0 = 0 \\\\\n",
    "v^0 = 0 \\\\\n",
    "v^{t+1}  = \\rho_1 v^t + (1-\\rho_1)\\nabla J(\\theta ^t) \\\\\n",
    "v^{t+1}_{corrected} = \\frac{v^{t+1}}{1-\\rho_1^t} \\\\\n",
    "s^{t+1}  = \\rho_2 s^t +(1-\\rho_2) [\\nabla J(\\theta ^t)-v_{t+1}]^2 \\\\\n",
    "s^{t+1}_{corrected} = \\frac{s^{t+1}}{1-\\rho_2^t} \\\\\n",
    "\\theta^{t+1} = \\theta ^t - \\frac{\\alpha}{\\sqrt{s^{t+1}_{corrected}}+\\epsilon}v^{t+1}_{corrected}\n",
    "$$\n",
    "\n",
    "其中，一般设$\\rho_1=0.9,\\rho_2=0.999$.$\\epsilon$也是防止分母过小或等于0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_adambelief(network) :\n",
    "    v = {}\n",
    "    s = {}\n",
    "\n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        ### START CODE HERE ###  #请初始化v和s\n",
    "        v[key] = np.zeros(np.shape(network.params[key]))\n",
    "        s[key] = np.zeros(np.shape(network.params[key]))\n",
    "        ### END CODE HERE ###\n",
    "            \n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1's shape: (784, 300)\n",
      "b1's shape: (300,)\n",
      "W2's shape: (300, 300)\n",
      "b2's shape: (300,)\n",
      "W3's shape: (300, 10)\n",
      "b3's shape: (10,)\n"
     ]
    }
   ],
   "source": [
    "network = TwoLayerNet(input_size=784, hidden_size=300, output_size=10)\n",
    "v, s = initialize_adambelief(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_adambelief(network, grads, learning_rate=0.001, epsilon=1e-7, beta1=0.9, beta2=0.999):\n",
    "    v_corrected = {}\n",
    "    s_corrected = {} \n",
    "    t = network.t #当前迭代次数\n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        ### START CODE HERE ### \n",
    "                \n",
    "        v[key] = beta1*v[key]+(1-beta1)*grads[key]\n",
    "        v_corrected[key] = v[key]/(1-beta1**t)\n",
    "        \n",
    "        s[key] = beta2*s[key]+(1-beta2)*(grads[key]-v[key])*(grads[key]-v[key])\n",
    "        s_corrected[key] = s[key]/(1-beta2**t)\n",
    "        \n",
    "        network.params[key] -= learning_rate/(np.sqrt(s_corrected[key])+epsilon)*v_corrected[key]\n",
    "        ### END CODE HERE ###\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc:0.229883\tTest acc:0.226300\n",
      "Train acc:0.960717\tTest acc:0.955500\n",
      "Train acc:0.970600\tTest acc:0.965000\n",
      "Train acc:0.979550\tTest acc:0.968900\n",
      "Train acc:0.985500\tTest acc:0.974000\n",
      "Train acc:0.987800\tTest acc:0.976200\n",
      "Train acc:0.989600\tTest acc:0.974900\n",
      "Train acc:0.991533\tTest acc:0.977100\n",
      "Train acc:0.990117\tTest acc:0.976700\n",
      "Train acc:0.993717\tTest acc:0.979300\n",
      "Train acc:0.993467\tTest acc:0.979100\n",
      "Train acc:0.994500\tTest acc:0.979400\n",
      "Train acc:0.992783\tTest acc:0.974000\n",
      "Train acc:0.996283\tTest acc:0.979000\n",
      "Train acc:0.995833\tTest acc:0.978000\n",
      "Train acc:0.995383\tTest acc:0.978100\n",
      "Train acc:0.996683\tTest acc:0.977900\n"
     ]
    }
   ],
   "source": [
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "train_network(network, update_parameters_with_adambelief, iters_num, train_size, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 总结\n",
    "本次实验完整搭建了一个三层的全连接网络，使用了各种梯度更新优化算法训练MNIST数据集。  \n",
    "或许你可以试试通过调整网络参数和超参数再把准确度提高一点？（记得记录每次训练时的数据结果）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "8ea307f64f28ee5e082e49b3665309add68f7312938244f4050a2f009b4177f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
